{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import Best_Sigma as BS\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error \n",
    "from numpy import  loadtxt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the DataSet from the Input folder\n",
    "\n",
    "X_ds = loadtxt('../Inputs/train_x.csv', delimiter=',')\n",
    "Y_new = loadtxt('../Inputs/train_y.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape and split the data in train and test set\n",
    "\n",
    "X_ds = X_ds.reshape((970,1,5))\n",
    "Y_new = Y_new.reshape((970,1,4900))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ds, Y_new, test_size=0.2, random_state = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adquire the trained model from the Models folder \n",
    "\n",
    "m_2LSTM = load_model('../Models/Final_resutls_2LSTM.h5')\n",
    "m_3LSTM = load_model('../Models/3LSTM_4900_BM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the models predictions \n",
    "\n",
    "pred_2 = m_2LSTM.predict(X_test)\n",
    "pred_3 = m_3LSTM.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models\n",
    "\n",
    "scores_2 = m_2LSTM.evaluate(X_test, y_test, verbose=0)\n",
    "scores_3 = m_3LSTM.evaluate(X_test, y_test, verbose=0)\n",
    "print('Evaluate: ', scores_2)\n",
    "print('Evaluate: ', scores_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best value of the sigma in the 2LSTM for APD50, APD75, APD90 and APDall \n",
    "# using the best_gausian function from the Best_Sigma.py file\n",
    "\n",
    "(MBPE_apd50_pa_2, MBPE_apd75_pa_2, MBPE_apd90_pa_2, MBPE_apd_pa_2,\n",
    " Sigma_value_min_apd50_2, MBPE_apd50_ga_2, Sigma_value_min_apd75_2, MBPE_apd75_ga_2, \n",
    " Sigma_value_min_apd90_2, MBPE_apd90_ga_2, Sigma_value_min_apdall_2, MBPE_apd_ga_2) = BS.best_gausian(pred_2)\n",
    "\n",
    "#Print the error for each occasion \n",
    "\n",
    "print(f\"MBPE of APD50 between Actual and Predicted: {MBPE_apd50_pa_2}\")\n",
    "print(f\"MBPE of APD75 between Actual and Predicted: {MBPE_apd75_pa_2}\")\n",
    "print(f\"MBPE of APD90 between Actual and Predicted: {MBPE_apd90_pa_2}\")\n",
    "print(f\"MBPE of APD ALL between Actual and Predicted: {MBPE_apd_pa_2}\\n\")\n",
    "print(f\"MBPE of APD50 between Actual and Best Gausian performance: {MBPE_apd50_ga_2}, Sigma Value: {Sigma_value_min_apd50_2}\")\n",
    "print(f\"MBPE of APD75 between Actual and Best Gausian performance: {MBPE_apd75_ga_2}, Sigma Value: {Sigma_value_min_apd75_2}\")\n",
    "print(f\"MBPE of APD90 between Actual and Best Gausian performance: {MBPE_apd90_ga_2}, Sigma Value: {Sigma_value_min_apd90_2}\")\n",
    "print(f\"MBPE of APD  ALL between Actual and Best Gausian performance: {MBPE_apd_ga_2}, Sigma Value: {Sigma_value_min_apdall_2}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best value of the sigma in the 3LSTM for APD50, APD75, APD90 and APDall \n",
    "# using the best_gausian function from the Best_Sigma.py file\n",
    "\n",
    "(MBPE_apd50_pa_3, MBPE_apd75_pa_3, MBPE_apd90_pa_3, MBPE_apd_pa_3,\n",
    " Sigma_value_min_apd50_3, MBPE_apd50_ga_3, Sigma_value_min_apd75_3, MBPE_apd75_ga_3, \n",
    " Sigma_value_min_apd90_3, MBPE_apd90_ga_3, Sigma_value_min_apdall_3, MBPE_apd_ga_3) = BS.best_gausian(pred_3)\n",
    "\n",
    "#Print the MBPE for each occasion \n",
    "\n",
    "print(f\"MBPE of APD50 between Actual and Predicted: {MBPE_apd50_pa_3}\")\n",
    "print(f\"MBPE of APD75 between Actual and Predicted: {MBPE_apd75_pa_3}\")\n",
    "print(f\"MBPE of APD90 between Actual and Predicted: {MBPE_apd90_pa_3}\")\n",
    "print(f\"MBPE of APD ALL between Actual and Predicted: {MBPE_apd_pa_3}\\n\")\n",
    "print(f\"MBPE of APD50 between Actual and Best Gausian performance: {MBPE_apd50_ga_3}, Sigma Value: {Sigma_value_min_apd50_3}\")\n",
    "print(f\"MBPE of APD75 between Actual and Best Gausian performance: {MBPE_apd75_ga_3}, Sigma Value: {Sigma_value_min_apd75_3}\")\n",
    "print(f\"MBPE of APD90 between Actual and Best Gausian performance: {MBPE_apd90_ga_3}, Sigma Value: {Sigma_value_min_apd90_3}\")\n",
    "print(f\"MBPE of APD  ALL between Actual and Best Gausian performance: {MBPE_apd_ga_3}, Sigma Value: {Sigma_value_min_apdall_3}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list with the best sigma value for APD50, APD75, APD90, APDall performaces\n",
    "sigma_2=[Sigma_value_min_apd50_2, Sigma_value_min_apd75_2, Sigma_value_min_apd90_2, Sigma_value_min_apdall_2]\n",
    "sigma_3=[Sigma_value_min_apd50_3, Sigma_value_min_apd75_3, Sigma_value_min_apd90_3, Sigma_value_min_apdall_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adquire the most frequent value of a sigma using the most_frequent function from the Best_Sigma.py file\n",
    "sigma_2 = BS.most_frequent(sigma_2)\n",
    "sigma_3 = BS.most_frequent(sigma_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the numpy \n",
    "\n",
    "res_2LSTM = np.zeros((194,4900))\n",
    "res_3LSTM = np.zeros((194,4900))\n",
    "pred_2 = pred_2.reshape((194,4900))\n",
    "pred_3 = pred_3.reshape((194,4900))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply gaussian filter in the models predictions using the best value for sigma  \n",
    "\n",
    "for i in range(pred_2.shape[0]):\n",
    "    res_2LSTM[i] = gaussian_filter1d(pred_2[i], sigma=sigma_2)\n",
    "    res_3LSTM[i] = gaussian_filter1d(pred_3[i], sigma=sigma_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape Y_test\n",
    "\n",
    "y_test = y_test.reshape((194,4900))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b47e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the performance of the model using Mean Squered Error \n",
    "\n",
    "scores_2 = mean_squared_error(y_test, res_2LSTM)\n",
    "scores_3 = mean_squared_error(y_test, res_3LSTM)\n",
    "print('MSE for the 2 LSTM Model: ', scores_2)\n",
    "print('MSE for the 2 LSTM Model: ', scores_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot some results from the 2LSTM Model\n",
    "\n",
    "fig = plt.figure(figsize = (25,5))\n",
    "\n",
    "#  Categorical Data\n",
    "a = 1 # number of rows\n",
    "b = 4  # number of columns\n",
    "c = 1  # initialize plot counter\n",
    "for i in range(0,194,50):\n",
    "    plt.subplot(a,b,c)\n",
    "    plt.plot(res_2LSTM[i].T)\n",
    "    plt.plot(y_test[i].T)\n",
    "    plt.ylabel('$V_m$ (mV)')\n",
    "    plt.legend([\"Predicted\",\"Actual\"])\n",
    "    plt.title('The 2 LSTM Model Results')\n",
    "    c = c + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot some results from the 3LSTM Model\n",
    "\n",
    "fig = plt.figure(figsize = (25,5))\n",
    "\n",
    "#  Categorical Data\n",
    "a = 1 # number of rows\n",
    "b = 4  # number of columns\n",
    "c = 1  # initialize plot counter\n",
    "\n",
    "for i in range(0,194,50):\n",
    "    plt.subplot(a,b,c)\n",
    "    # predicted[i] = predicted[i].reshape((8000,1))\n",
    "    # y_test[i] = y_test[i].reshape((8000,1))\n",
    "    plt.plot(p_gau[i].T)\n",
    "    plt.plot(res_3LSTM[i].T)\n",
    "    plt.ylabel('$V_m$ (mV)')\n",
    "    plt.legend([\"Predicted\",\"Actual\"])\n",
    "    plt.title('The 3 LSTM Model Result')\n",
    "    c = c + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
